---
title: 'Tarea 1: Módulo VI APRENDIZAJE NO SUPERVISADO'
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(root.dir = "../..")
```


**Integrantes:**

* Luis Rodrigo Santamaría Rodríguez
* Jonathan Vargas González
* Marisol Estrada Téllez

Para la presente tarea se trabajará con la siguiente base da datos llamada *water potability.csv* 
la cual contiene medidas físico-químicas y evaluaciones de la calidad del agua relacionadas con su potabilidad, es decir, la idoneidad para el consumo humano. Proporciona información sobre los parámetros de la calidad del agua y ayudar a
determinar si es potable o no. 

Cada renglón representa una muestra de agua con atributos específicos y la columna *Potability* indica si el agua es o no apta para el consumo humano. 

Las variables medidas son: 

- ph: Nivel de ph.
- Dureza (Hardness): Dureza, medida del contenido mineral. 
- Sólidos (Solids): Total de sólidos disueltos.
- Cloraminas (Chloramines): Concentración de cloraminas.
- Sulfato (Sulfate): Concentración de sulfato.
- Conductividad (Conductivity): Conductividad eléctrica.
- Carbono organico (Organic carbon): Contenido de carbono organic,
- Trihalometanos (Trihalomethanes): Concentración de trihalometano,
- Turbidez (Turbidity): Nivel de turbidez, medida de la claridad.
- Potabilidad (Potability): Indica la potabilidad con valores de 1 (potable) y 0 (no potable).

El objetivo de esta tarea es usar técnicas de reducción de dimensionalidad para facilitar su análisis, visualización y comprensión. Los métodos seleccionados para este propósito son:

* **PCA** (Análisis de Componentes Principales) 
* **t-SNE** (t-distributed Stochastic Neighbor Embedding)
* **UMAP** (Uniform Manifold Approximation and Projection)

**Iniciamos revisando nuestro dataset y haciendo un análisis exploratorio.**

```{r message=FALSE, warning=FALSE, include=FALSE}
###Librerias
library(tidymodels)
library(tidyverse)
library(knitr)
library(kableExtra)
library(GGally)
library(psych)
library(ggplot2)
library(broom)
library(Hmisc)
library(scales)
library(autoplot)
library(recipes)
library(rgl)
library(plotly)
library(ggforce)
library(tidytext)
library(FactoMineR)  
library(factoextra)
library(ggord)
library(mice)
library(ggfortify)
theme_set(theme_bw(16))
```

Cargamos nuestra base de datos.

```{r message=FALSE, warning=FALSE}
# Cargamos la base de datos
df<-read.csv("water_potability.csv") 
```

Veamos la dimensión de nuestra base, la cual tiene 3276 registros y 10 columnas.

```{r message=FALSE, warning=FALSE}
df %>% dim()
```

Checamos los tipos de datos que tenemos y algunos registros.
```{r message=FALSE, warning=FALSE}
df %>% glimpse()
df %>% head()
```

Podemos observar que tenemos algunos datos faltantes de pH Sulfatos, además, la variable "Potability" es la única variable que es de tipo "int".

Revisamos cuántos datos faltantes tenemos por cada columna.
```{r message=FALSE, warning=FALSE}
colSums(is.na(df))
```

Observemos que:

- Tenemos 491 datos faltantes en pH.
- Tenemos 781 datos faltantes en Sulfatos.
- Tenemos 162 datos faltantes en Trihalometanos.


Imputaremos los datos faltantes con ayuda de la función  `mice` usando el método PMM (Coincidencia de Medias Predictivas).

```{r message=FALSE, warning=FALSE, results="hide"}
imputed_data <- mice(df, m = 5, method = 'pmm', maxit = 5, seed = 123)
```

Vamos a poner en nuestra variable df los datos imputados.
```{r message=FALSE, warning=FALSE}
df <- complete(imputed_data, 1)
```

Comprobemos que ya no tenemos datos faltantes en nuestro dataframe.
```{r message=FALSE, warning=FALSE}
colSums(is.na(df))
```

### **Análisis exploratorio de datos**

A continuación creamos una tabla que nos muestra el mínimo, primer cuantil, la mediana, media, tercer quantil y el máximo de cada una de nuestras columnas.

```{r message=FALSE, warning=FALSE}
num.dat <- df %>% select_if(is.numeric)
# num.dat %>% glimpse()

apply(num.dat,2,function(x) round(summary(x),3)) %>% 
   kbl() %>%
  kable_styling(bootstrap_options = c("striped","hover","bordered")) %>% 
    kable_paper() %>%
  scroll_box(width = "100%", height = "320px")
```


Tomamos como variable de clasificación: potability, cabe mencionar que esto lo hacemos solo de referencia ya que, en aprendizaje supervisado no tenemos una variable de salida que nos ayude a categorizar, solo lo haremos para fines ilustrativos.

Revisamos cuántos tipos de agua son adecuados para el consumo humano y cuántos no lo son.
```{r message=FALSE, warning=FALSE}
df %>%
  group_by(Potability) %>%
  summarise (n = n()) %>%
  mutate(prop = n / sum(n)) %>%
ggplot(aes(df,x = Potability, y = n)) +
    theme_minimal() +
    geom_col(fill = c("#ec7d15", "#10b9ec")) +
    geom_text(aes(label = paste0(n, " | ", signif(n / nrow(df) * 100, digits = 4), '%')), nudge_y = 10) + ggtitle("Porcentajes de agua potable y no potable")

df %>%
  select(where(is.numeric)) %>%
  colMeans()

df1 <- df |> dplyr::select(where(is.numeric))
```
Podemos observar que hay 1998 registros que no son aptos para el consumo humano y 1278 registros si son aptos para el consumo humano.
Además, 60.99% de registros no son aptos para el consumo humano y un 39.01% sí lo son.

**Ahora, realizamos histogramas de cada una de nuestras columnas.**

**Boxplots:**
```{r, warning=FALSE, message=FALSE, include=FALSE}
df_long <- df %>% 
    pivot_longer(!Potability, names_to = "predictores", values_to = "values")
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
df_long %>% 
  ggplot(mapping = aes(x = Potability, y = values, fill = as.factor(predictores))) +
  geom_boxplot(alpha = 0.4)  + theme_light() + 
  facet_wrap(~ predictores, scales = "free", ncol = 4) +
  scale_color_viridis_d(option = "plasma", end = .7) +
  theme(legend.position = "none") +
  labs(title = "Comparación vía box-plot")
```

**Histogramas:**
```{r, echo =FALSE, message = FALSE, warning=FALSE}
df_long |> ggplot(mapping = aes(values, fill = Potability)) +
  geom_histogram(color = "white", alpha = 0.4) +
  theme_light() +
  scale_fill_manual(values = c("0" = "#ec7d15", "1" = "#10b9ec")) + 
  facet_wrap(~predictores, scales = "free", ncol= 4) +
  labs(title = "Variables Distribution") +
  labs(title = "Comparación vía histograma")

```

**Densidades:**
```{r, warning=FALSE, message=FALSE, echo =FALSE}
df_long |> ggplot(mapping = aes(values, fill = Potability)) +
  geom_density(color = "white", alpha = 0.4) +
  theme_light() +
  scale_fill_manual(values = c("0" = "#ec7d15", "1" = "#10b9ec")) +
  facet_wrap(~predictores, scales = "free", ncol= 4) +
  labs(title = "Distribución de variables") +
  labs(title = "Comparación a través de densidad")
```

**Gráficos de dispersión:**
```{r message=FALSE, warning=FALSE, echo = FALSE}
ggpairs(df, mapping = aes(color = Potability, alpha = 0.5),columns = seq(1,9), upper = "blank", diag = "blank") +
scale_colour_manual(values = c("0" = "#ec7d15", "1" = "#10b9ec")) + labs(title = "Comparación por gráficos de dispersión")
```

Veamos la estructura de correlación de Spearman que tienen nuestras variables.

```{r message=FALSE, warning=FALSE, echo=FALSE}
cor1_data<-cor(df[,-10],method="spearman")

ggcorrplot::ggcorrplot(corr = cor1_data,
                       type = "lower", 
                       show.diag = TRUE,
                       lab = TRUE, 
                       lab_size = 3) +
                       labs(title = "Correlación entre las variables")
```

Observamos que **no hay mucha correlación en nuestras variables** por lo que podemos intuir que la **reducción de dimensionalidad no se llevará a cabo con éxito** ya que PCA, hace una reducción de dimensión mientras haya una correlación fuerte entre nuestras variables.

```{r warning = FALSE, message=FALSE}
psych::KMO(cor1_data)
```
Además, tomando en cuenta la medida Kaiser-Meyer-Olkin, las variables no son muy adecuadas para reducir la dimensión por estar debajo de 0.5.


## **PCA**
Después de hacer un Análisis Exploratorio de nuestro Dataset podemos realizar la Reducción de dimensión utilizando el Método PCA (Componentes Principales).

Estandarizamos los datos y trataremos de reducir dimensionalidad con PCA.
```{r message=FALSE, warning=FALSE}
pca_fit <- df %>%
  select(where(is.numeric)) %>%
  scale() %>% #estandarizamos los datos
  prcomp()

#str(pca_fit)
```

Veamos el resultado de PCA.
```{r message=FALSE, warning=FALSE}
pca_fit
# Los componentes principales son 9, son pesos de nuestras variables
```

Observamos que los eigenvalores van decreciendo pero no son cercanos a cero, por lo que podemos decir que todos los eigenvalores tienen mucha varianza explicada.

```{r message=FALSE, warning=FALSE}
pca_fit %>%
  tidy("pcs") #%>% print(n=Inf)
```

Veamos una gráfica de codo la cual nos muestra como va bajando la varianza explicada por cada una de las Componentes Principales.

```{r message=FALSE, warning=FALSE, echo=FALSE}
pca_fit %>%
  tidy("pcs") %>%
  ggplot(aes(x=term, y=prop.variance, group = 1)) +
  geom_col(fill="#b229e9da", alpha=0.7) +
  geom_point(size=2) +
  geom_line(color="darkred", size=1.1) +
  scale_y_continuous(labels=scales::label_percent(),
                     breaks = scales::breaks_pretty(n=6))+
  labs(y= "Varianza explicada", title="Scree plot") +
  theme_minimal()
```

Cada una de las componentes tiene casi la misma varianza por lo que podemos intuir que no será posible graficar en R3 ni en R2 ya que **si solo nos quedamos con 2 o 3 Componentes Principales perderemos mucha varianza de las demás componentes**.

Veamos una gráfica ahora de la Varianza acumulativa.
```{r message=FALSE, warning=FALSE}
pca_fit %>%
  tidy("pcs") %>%
  ggplot(aes(x=term, y= cum.prop.variance, group = 1))+
  geom_col(fill="#b229e9da", alpha=0.7) +
  geom_point(size=2) +
  geom_line(color="darkred", size=1.1)+
  scale_y_continuous(labels=scales::label_percent(),
                     breaks = scales::breaks_pretty(n=6))+
  labs(y= "Varianza explicada acumulada",title="Scree plot")
```

```{r message=FALSE, warning=FALSE}
variance_exp <- pca_fit %>%  
  tidy("pcs") %>% 
  pull(prop.variance)
```

Vamos a realizar una gráfica con los dos primeros componentes.

Vamos a hacer uso de nuestra variable *Potability*.
```{r message=FALSE, warning=FALSE, echo=FALSE}
pca_fit %>%
  augment(df) %>%
  rename_with(function(x){gsub("[.]","",x)}) %>%
  ggplot(aes(x = PC1, y = PC2, color=Potability))+
  geom_point()+
  scale_color_manual(values = c("0" = "#ec7d15", "1" = "#10b9ec")) +
  labs(x = paste0("PC1: ",round(variance_exp[1]*100), "%"),
       y = paste0("PC2: ",round(variance_exp[2]*100), "%"))+
  labs(title="Gráfica de componentes principales")
```

Podemos ver que no nos separan bien los datos en los grupos que tenemos de potabilidad.

Gráfica de Boxplot de la variable *Potability* con el primer componente principal.
```{r message=FALSE, warning=FALSE, echo=FALSE}
pca_fit %>%
  augment(df) %>%
  rename_with(function(x){gsub("[.]","",x)}) %>%
  ggplot(aes(x = Potability, y = PC1, color=Potability))+
  geom_boxplot(outlier.shape = NA)+
  scale_color_manual(values = c("0" = "#ec7d15", "1" = "#10b9ec")) +
  geom_point(position=position_jitterdodge())+
  labs(y = paste0("PC1: ",round(variance_exp[1]*100), "%"))+
  theme(legend.position = "top")
```

Gráfica de Boxplot de la variable *Potability* con el segundo componente principal.
```{r message=FALSE, warning=FALSE, echo=FALSE}
pca_fit %>%
  augment(df) %>%
  rename_with(function(x){gsub("[.]","",x)}) %>%
  ggplot(aes(x = Potability, y = PC2, color=Potability))+
  geom_boxplot(outlier.shape = NA)+
  scale_color_manual(values = c("0" = "#ec7d15", "1" = "#10b9ec")) +

  geom_point(position=position_jitterdodge())+
  labs(y = paste0("PC2: ",round(variance_exp[2]*100), "%"))+
  theme(legend.position = "top")
# Los boxplots de la primera y de la segunda componente son muy similares
```

Notemos que ambas gráficas son similares los que nos indica que probablemente la diferencia en varianza explicada no es tan grande. Esto puede sugerir que ambas componentes son relevantes para entender la estructura del conjunto de datos. Pero como ya vimos anteriormente, esto ocurre también con las otras variables.

```{r message=FALSE, warning=FALSE, include=FALSE}
wdbc_recipe <-
  recipe(~., data = df) %>% 
  update_role(Potability, new_role = "id") %>% 
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), id = "pca") %>% 
  prep()
```

Obtenemos la carga de cada variable en el componente principal correspondiente.
```{r message=FALSE, warning=FALSE, include=FALSE}
wdbc_pca <- 
  wdbc_recipe %>% 
  tidy(id = "pca") 
```

**Veremos la contribución de cada variable a los componentes principales:**
Nota: Azul oscuro para contribución positiva y azul claro para contribución negativa.

```{r message=FALSE, warning=FALSE, echo=FALSE}
wdbc_pca %>%
  mutate(terms = tidytext::reorder_within(terms, 
                                          abs(value), 
                                          component)) %>%
  ggplot(aes(abs(value), terms, fill = factor(value > 0, levels = c(FALSE, TRUE), labels = c("Negativa", "Positiva")))) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  tidytext::scale_y_reordered() +
  scale_fill_manual(values = c("#b6dfe2", "#0A537D")) +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Contribución"
  ) 
```

```{r message=FALSE, warning=FALSE, results="hide", include=FALSE}
res.pca <- PCA(df[, 1:9],  scale.unit=TRUE) 
```

```{r message=FALSE, warning=FALSE}
fviz_pca_var(res.pca,
             alpha.var = "contrib",
             col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             title = 'Influencia de las variables en PCA1 y PCA2',
             repel = TRUE)
```


Ahora realizaremos un gráfico 3D con los primeros tres Componentes Principales.
```{r message=FALSE, warning=FALSE, echo =FALSE}
df1 <- cbind(df, pca_fit$x)
head(df1)

g.df1 <- plot_ly(df1, x = ~PC1, y = ~PC2, z = ~PC3, color = ~Potability, colors =c("0" = "#ec7d15", "1" = "#10b9ec"), 
marker = list(size = 5))
g.df1 <- g.df1 %>% add_markers()
g.df1 <- g.df1 %>% layout(scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3')))

g.df1

```

Notemos que la mayoria de los puntos están concentrados en el centro. Por lo que podemos concluir que nuevamente no logró hacer una separación clara.

Probemos con los siguientes métodos.

```{r message=FALSE, warning=FALSE}

fig <- plot_ly(df1, x = ~PC1, y = ~PC2, z = ~PC3, color = ~Potability, colors = c('#BF382A', '#0C4B8E'))
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'PC1'),
                     yaxis = list(title = 'PC2'),
                     zaxis = list(title = 'PC3')))
fig
```




### **tSNE**



### **UMAP**

